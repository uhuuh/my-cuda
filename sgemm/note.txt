

为了方便处理带batch维度的tensor，在linear中一般是x乘上w

cublas输入都是列主序存储，如果要计算a @ b，则实际调用gemm传入b和a

cutlass支持shape和stride, 不支持step, 因为step可以吸收到stride中

输入A, B, C, alpha, beta, M, N, K, 其中A是M*K, B是K*N, C是M*N, ABC都使用列主序存储, 计算C = alpha * A @ B + beta * C

- native: grid(M / 16, N / 16) block(16, 16) 每个thread计算C中一个位置的最终值
- smem_cache: grid(M / 16, N / 16) block(16, 16) 每个block维护一个在smem上的tile, 减少gmem加载次数
- rmem_cache: grid(M / 256, N / 256) block(4, 4)
- mem_coalesce:
- vector_memory_access
- double buffer
- wmma
- mma
- cutlass
- triton
- tilelang

- 什么参数应该设置为模板参数
- 什么时候应该使用sync
- rmem cache是如何实现的, 比如数据搬运量和计算量该如何决定
- 实际上gemm应该考虑更多细节，比如当shape不能被tile size整除，比如tile size应该是二维的
- 大矩阵相乘的时候，累加操作是有可能有精度问题吗？
- 调用一个kernel，有一些预处理操作，这些操作应该放在c++，还是放在python中，如果放在python中，一些模板参数如何传递。一些参数必须是模板参数，比如设置share memory的时候需要shape是编译器常量


## 调试工具
- Compute Sanitizer
- cuda gdb
- printf

## 传入行主序输入，但是gemm处理列主序

torch 创建tensor默认行主序

tensor转置不生成新tensor，只改变view，但是is_contiguous()返回False, 因为这里的contiguous指的按行访问时连续的

## cuda gdb 调试
可以自定义一个打印矩阵的函数
```
define pm
    set $A   = $arg0
    set $M   = $arg1
    set $N   = $arg2
    set $lda = $arg3

    set $i = 0
    while ($i < $M)
        set $j = 0
        printf "row %d: ", $i
        while ($j < $N)
            printf "%6.2f ", $A[$i + $j * $lda]
            set $j = $j + 1
        end
        printf "\n"
        set $i = $i + 1
    end
end
```

