

为了方便处理带batch维度的tensor，在linear中一般是x乘上w

cublas输入都是列主序存储，如果要计算a @ b，则实际调用gemm传入b和a

cutlass支持shape和stride, 不支持step, 因为step可以吸收到stride中

输入A, B, C, alpha, beta, M, N, K, 其中A是M*K, B是K*N, C是M*N, ABC都使用列主序存储, 计算C = alpha * A @ B + beta * C

- native: grid(M / 16, N / 16) block(16, 16) 每个thread计算C中一个位置的最终值
- smem_cache: grid(M / 16, N / 16) block(16, 16) 每个block维护一个在smem上的tile, 减少gmem加载次数
- rmem_cache: grid(M / 256, N / 256) block(4, 4)
- mem_coalesce:
- vector_memory_access
- double buffer
- wmma
- mma
- cutlass
- triton
- tilelang

- 什么参数应该设置为模板参数
- 什么时候应该使用sync
- rmem cache是如何实现的, 比如数据搬运量和计算量该如何决定
- 实际上gemm应该考虑更多细节，比如当shape不能被tile size整除，比如tile size应该是二维的
- 大矩阵相乘的时候，累加操作是有可能有精度问题吗？
- 调用一个kernel，有一些预处理操作，这些操作应该放在c++，还是放在python中，如果放在python中，一些模板参数如何传递。一些参数必须是模板参数，比如设置share memory的时候需要shape是编译器常量
- 当cuda kernel绑定torch算子时，需要引入torch头文件，由于torch头文件有很多，clion解析时非常耗时，并且没有解析完就没有语法只能提示
- 似乎应该，将cuda kernel注册为torch算子，和输入检查部分挪到python侧，减轻c++侧的重复代码
- 还需要看一下占用度，设置grid和block相关的内容
- warp tiling，warp负责的shape不一样，从gmem到smem搬运的数据量也不一样，比如（1，32）就是32（由于合并访存的存在，所有warp线程都加载A中同一个元素），而（4,8）就是12。这说明从gmem到smeme的加载，除了考虑访存连续外，还是考虑合并
一次gmem内存传输最多128B，也就是32个float，当warp中多个thread读取的数据大于128B时，先处理前面thread，如果每个thread处理连续数据范围内的一部分时，这可以保证连续的数据存取
如果warp tile shape是（4,8），当每个thread处理8*8小块时，4个thread的8个float正好可以覆盖一次gmem传输
参考 [施工中] CUDA GEMM 理论性能分析与 kernel 优化 - 李少侠的文章 - 知乎 https://zhuanlan.zhihu.com/p/441146275
从smem上避免bank conflict的角度，warp的shape应该是（4,8），每个thread负责计算8*8小块。从gmem合并访存的角度，tile的shape最小的那一维度至少是32 float，因为一次gmem传输最多128B，也就是32个float
- 为什么能找到的实现，都不满足tile的最小维度最少是32？
- 从smem读取到rmem有bank conflict的概念，那么从gmem传输到smem有bank conflict的概念吗？---llm说没有
- 好像一个block一般16*16个线程，一般sm中有四个子分区和最多2048个thread。这样正好可以放下8个warp，每个分区执行2个warp，warp之间勉强可以计算与访存掩盖
- 使用`#pragma unroll`好像还挺有用的，提升了不少
- 合并访存和bank冲突，好像是都不能解决的
- 之前是gmem->rmem->smem，将rmem到smem这一步挪到这里了。这样操作有意义吗？一个是不是说是顺序执行吗？前面gmem的加载和计算可以并行吗？然后后面设备好像支持直接从gmem到smem了。这个提升巨大
- 写法一样，但是性能差挺多，一个点是有L1TEX存取，另外一个smem和寄存器使用了更多，导致占用度低
- 为什么不能强制所有变量分配到寄存器上，否则就报错
- 好像把小块移动操作去掉，直接取小块，也有一些性能提升

## 调试工具
- Compute Sanitizer
- cuda gdb
- printf

## 传入行主序输入，但是gemm处理列主序

torch 创建tensor默认行主序

tensor转置不生成新tensor，只改变view，但是is_contiguous()返回False, 因为这里的contiguous指的按行访问时连续的

## cuda gdb 调试
可以自定义一个打印矩阵的函数
```
define pm
    set $A   = $arg0
    set $M   = $arg1
    set $N   = $arg2
    set $lda = $arg3

    set $i = 0
    while ($i < $M)
        set $j = 0
        printf "row %d: ", $i
        while ($j < $N)
            printf "%6.2f ", $A[$i + $j * $lda]
            set $j = $j + 1
        end
        printf "\n"
        set $i = $i + 1
    end
end
```

