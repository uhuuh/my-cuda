# test_sgemm.py
import os
import torch
import matplotlib.pyplot as plt
import statistics
import numpy as np

from torch.utils.cpp_extension import load
from contextlib import contextmanager
from types import SimpleNamespace
from tqdm import tqdm

# ===============================
# 1. CUDA è®¡æ—¶å™¨
# ===============================
@contextmanager
def cuda_timer():
    result = SimpleNamespace(ms=None)
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    torch.cuda.synchronize()
    start.record()
    yield result
    end.record()
    torch.cuda.synchronize()
    result.ms = start.elapsed_time(end)

# ===============================
# 2. CUDA ç®—å­åŠ è½½å™¨ (JIT ç¼–è¯‘)
# ===============================
_OP_CACHE = {}

def load_op(op_type: str, debug=False):
    """åŠ è½½å¹¶ç¼–è¯‘æŒ‡å®šçš„ CUDA ç®—å­"""
    if op_type in _OP_CACHE:
        return _OP_CACHE[op_type]

    cc_major, cc_minor = torch.cuda.get_device_capability()
    os.environ["TORCH_CUDA_ARCH_LIST"] = f"{cc_major}.{cc_minor}"

    build_root = os.path.join(os.getcwd(), "build")
    build_subdir_name = f"op={op_type}___debug={debug}___"
    build_dir = os.path.join(build_root, build_subdir_name)
    os.makedirs(build_dir, exist_ok=True)

    extra_cuda_cflags = ["-O3", "-lineinfo"] if not debug else ["-O0", "-lineinfo", "-G", "-g"]

    # å‡è®¾æºæ–‡ä»¶å‘½åä¸º sgemm_{op_type}.cu
    source_file = f"{op_type}.cu"
    if not os.path.exists(source_file):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æºæ–‡ä»¶: {source_file}")

    lib = load(
        name=f"{op_type}",
        sources=[source_file],
        build_directory=build_dir,
        verbose=True,
        extra_cuda_cflags=extra_cuda_cflags,
    )

    op = getattr(lib, f"{op_type}")
    _OP_CACHE[op_type] = op
    return op

# ===============================
# 3. æ ¸å¿ƒåŸºå‡†æµ‹è¯•é€»è¾‘ (é‡æ„ç‰ˆ)
# ===============================
def benchmark_single_run(op_fn, A, B, warmup=5, samples=10, reference_C=None):
    """åœ¨ç»™å®šè¾“å…¥ A, B ä¸Šæµ‹è¯•ç®—å­æ€§èƒ½å¹¶éªŒè¯ç»“æœ"""
    # é¢„çƒ­
    for _ in range(warmup):
        op_fn(A, B)

    # è®¡æ—¶
    times = []
    for _ in range(samples):
        with cuda_timer() as t:
            C = op_fn(A, B)
        times.append(t.ms)

    # æ­£ç¡®æ€§éªŒè¯
    if reference_C is not None:
        max_diff = (C - reference_C).abs().max().item()
        # å¯¹äº FP32ï¼Œå…è®¸ä¸€å®šçš„æµ®ç‚¹è¯¯å·®
        assert max_diff < 1e-2, f"ç²¾åº¦éªŒè¯å¤±è´¥! Max diff: {max_diff:.6e}"

    return {"times": times, "result": C if reference_C is None else None}

def run_benchmark_plot(baseline_fn, ops_dict, warmup=5, samples=10):
    """å°ºå¯¸ä¼˜å…ˆæµ‹è¯•ï¼šæ¯ä¸ª shape ä¸‹å…±äº«è¾“å…¥ï¼Œæ‰§è¡Œæ‰€æœ‰ç®—å­"""
    GROUPS = {
        "Square GEMM": [
            (2048, 2048, 2048),
            (4096, 4096, 4096),
            (8192, 8192, 8192),
        ],
        "M,N Large / K Small": [
            (8192, 1024, 8192),
            (16384, 1024, 8192),
        ],
        "M Small / K,N Large": [
            (1024, 8192, 8192),
            (1024, 16384, 8192),
        ],
    }

    op_names = list(ops_dict.keys())
    # å­˜å‚¨ç»“æ„: {group_name: {impl_name: {(M,K,N): {times:[], gflops:float}}}}
    all_results = {group: {name: {} for name in (["torch"] + op_names)} for group in GROUPS}

    print("\nğŸš€ å¼€å§‹ SGEMM ç»Ÿä¸€åŸºå‡†æµ‹è¯• (å…±äº«è¾“å…¥æ¨¡å¼)...")

    for group_name, shapes in GROUPS.items():
        print(f"\næµ‹è¯•ç»„: {group_name}")
        for shape in tqdm(shapes, desc="æ­£åœ¨å¤„ç†ä¸åŒå°ºå¯¸"):
            M, K, N = shape

            # --- 1. å‡†å¤‡è¯¥å°ºå¯¸ä¸‹å…±äº«çš„è¾“å…¥ ---
            A = torch.randn(M, K, device="cuda")
            B = torch.randn(K, N, device="cuda")
            gflops_val = 2.0 * M * K * N / 1e9

            # --- 2. å…ˆè¿è¡Œ PyTorch (cuBLAS) è·å–å‚è€ƒæ—¶é—´ä¸ç»“æœ ---
            torch_res = benchmark_single_run(baseline_fn, A, B, warmup, samples)
            all_results[group_name]["torch"][shape] = {
                "times": torch_res["times"],
                "gflops": gflops_val
            }
            ref_C = torch_res["result"]

            # --- 3. è¿è¡Œæ‰€æœ‰è‡ªå®šä¹‰ç®—å­å¹¶è¿›è¡ŒéªŒè¯ ---
            for op_name, op_fn in ops_dict.items():
                try:
                    res = benchmark_single_run(op_fn, A, B, warmup, samples, reference_C=ref_C)
                    all_results[group_name][op_name][shape] = {
                        "times": res["times"],
                        "gflops": gflops_val
                    }
                except Exception as e:
                    print(f"\nâŒ ç®—å­ {op_name} åœ¨å°ºå¯¸ {shape} å‡ºé”™: {e}")

            # --- 4. æ˜¾å­˜æ¸…ç†é˜²æ­¢ OOM ---
            del A, B, ref_C
            torch.cuda.empty_cache()

    # æ‰§è¡Œç»˜å›¾
    plot_all_groups(all_results, op_names)

# ===============================
# 4. å¯è§†åŒ–å‡½æ•° (å¢å¼ºç‰ˆ)
# ===============================
def plot_all_groups(all_results, op_names):
    """ç»˜åˆ¶é«˜å¯¹æ¯”åº¦å›¾è¡¨ï¼Œå¼ºåŒ–åˆ—åæ ‡ç½‘æ ¼"""
    def compute_stats(results):
        stats = {"x": [], "mean": [], "min": [], "max": []}
        for shape in sorted(results.keys()):
            data = results[shape]
            gflops = data["gflops"]
            # æ€§èƒ½è®¡ç®—: GFLOPs / (ms / 1000)
            perfs = [gflops / (t / 1e3) for t in data["times"]]
            stats["x"].append(gflops)
            stats["mean"].append(statistics.mean(perfs))
            stats["min"].append(min(perfs))
            stats["max"].append(max(perfs))
        return stats

    num_groups = len(all_results)
    cols = 2
    rows = (num_groups + cols - 1) // cols

    # é…è‰²æ–¹æ¡ˆï¼šé²œè‰³çš„éœ“è™¹è‰²
    vibrant_colors = ["#FF0055", "#00E676", "#2979FF", "#FF9100", "#D500F9", "#00E5FF"]
    styles = {"torch": {"fmt": "s--", "color": "#000000", "label": "PyTorch (cuBLAS)"}}
    for i, name in enumerate(op_names):
        styles[name] = {
            "fmt": "o-",
            "color": vibrant_colors[i % len(vibrant_colors)],
            "label": name.replace("_", " ").title()
        }

    fig, axes = plt.subplots(rows, cols, figsize=(cols * 8, rows * 6), squeeze=False)
    fig.patch.set_facecolor('#F8F9FA')

    for ax, (group_name, group_results) in zip(axes.flat, all_results.items()):
        ax.set_facecolor('#FFFFFF')

        # --- å¼ºåŒ–ç½‘æ ¼ä¸åˆ—åæ ‡ ---
        ax.minorticks_on()
        # ä¸»ç½‘æ ¼çº¿ï¼ˆä¸»è¦åæ ‡ï¼‰
        ax.grid(True, which='major', linestyle='-', linewidth=0.8, color='#D1D1D1')
        # æ¬¡ç½‘æ ¼çº¿ï¼ˆå³â€œåˆ—åæ ‡â€æ„Ÿï¼‰
        ax.grid(True, which='minor', linestyle=':', linewidth=0.5, color='#EAEAEA')

        for impl_name in ["torch"] + op_names:
            if not group_results[impl_name]: continue
            stats = compute_stats(group_results[impl_name])

            yerr = [[stats["mean"][i] - stats["min"][i] for i in range(len(stats["mean"]))],
                    [stats["max"][i] - stats["mean"][i] for i in range(len(stats["mean"]))]]

            ax.errorbar(
                stats["x"], stats["mean"], yerr=yerr,
                fmt=styles[impl_name]["fmt"], color=styles[impl_name]["color"],
                capsize=5, capthick=1.5, label=styles[impl_name]["label"],
                linewidth=2.5, markersize=8, alpha=0.9
            )

        ax.set_title(group_name, fontsize=15, fontweight='bold', pad=15)
        ax.set_xlabel("Workload (GFLOPs)", fontsize=11, fontweight='bold')
        ax.set_ylabel("Throughput (GFLOPs/s)", fontsize=11, fontweight='bold')
        ax.legend(loc='best', frameon=True, shadow=True, fontsize=9)

    # éšè—ç©ºå­å›¾
    for ax in axes.flat[num_groups:]:
        ax.set_visible(False)

    fig.suptitle("SGEMM Performance Comparison (Shared Inputs per Shape)", fontsize=18, fontweight='bold', y=0.98)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    out_file = "sgemm_benchmark_vibrant.png"
    plt.savefig(out_file, dpi=200, bbox_inches='tight')
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼å›¾è¡¨å·²ä¿å­˜è‡³: {out_file}")
    plt.show()

# ===============================
# 5. ä¸»ç¨‹åºå…¥å£
# ===============================
if __name__ == "__main__":
    if not torch.cuda.is_available():
        print("é”™è¯¯: æœªæ£€æµ‹åˆ° CUDA è®¾å¤‡")
        exit()

    print(f"å½“å‰ GPU è®¾å¤‡: {torch.cuda.get_device_name(0)}")

    # è°ƒè¯•æ¨¡å¼æ ‡å¿—
    is_debug = False

    # å®šä¹‰è¦æµ‹è¯•çš„ç®—å­æ–‡ä»¶åï¼ˆä¸å« .cu åç¼€ï¼‰
    op_types = ["sgemm_native", "sgemm_smem_cache", "sgemm_rmem_cache", "sgemm_double_buffer", "sgemm_memory_coalesce"]

    ops_dict = {}
    print("\næ­£åœ¨ç¼–è¯‘å¹¶åŠ è½½ CUDA ç®—å­...")
    for op_type in op_types:
        try:
            ops_dict[op_type] = load_op(op_type, debug=is_debug)
            print(f"  - {op_type} åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"  - {op_type} åŠ è½½å¤±è´¥: {e}")

    if not ops_dict:
        print("æœªåŠ è½½ä»»ä½•è‡ªå®šä¹‰ç®—å­ï¼Œä»…è¿è¡Œ PyTorch åŸºå‡†ã€‚")

    # è¿è¡Œå®Œæ•´çš„æ€§èƒ½åˆ†æ
    run_benchmark_plot(
        baseline_fn=torch.matmul, # ç›´æ¥ä½¿ç”¨ torch.matmul ä½œä¸ºåŸºå‡†
        ops_dict=ops_dict,
        warmup=5,
        samples=10
    )